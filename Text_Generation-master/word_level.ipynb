{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Beginners Guide to Text Generation using GRUs**\n",
    "\n",
    "Text Generation is a type of Language Modelling problem. Language Modelling is the core problem for a number of of natural language processing tasks such as speech to text, conversational system, and text summarization. A trained language model learns the likelihood of occurrence of a word based on the previous sequence of words used in the text. Language models can be operated at character level, n-gram level, sentence level or even paragraph level. In this notebook, I will explain how to create a language model for generating natural language text by implement and training state-of-the-art Recurrent Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "import unidecode\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    This process, however, afforded me no means of...\n",
       "2    In his left hand was a gold snuff box, from wh...\n",
       "6    The astronomer, perhaps, at this point, took r...\n",
       "7          The surcingle hung in ribands from my body.\n",
       "8    I knew that you could not say to yourself 'ste...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../input/train.csv')\n",
    "author = train_df[train_df['author'] == 'EAP'][\"text\"]\n",
    "author[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2802"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = list(author[:100])\n",
    "def joinStrings(text):\n",
    "    return ' '.join(string for string in text)\n",
    "text = joinStrings(text)\n",
    "# text = [item for sublist in author[:5].values for item in sublist]\n",
    "len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(nltk.corpus.stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "        stop_free = \" \".join([i for i in doc.split() if i not in stop])\n",
    "        punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n",
    "        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "        return normalized\n",
    "test_sentence = clean(text).lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **N-Gram Language Modeling**\n",
    "\n",
    "Recall that in an n-gram language model, given a sequence of words w, we want to compute.\n",
    "                                      * P(wi|wi−1,wi−2,…,wi−n+1)                                                     \n",
    "Where wi is the ith word of the sequence.                                                                              here we will take n=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['this', 'process'], 'however'), (['process', 'however'], 'afforded'), (['however', 'afforded'], 'mean')]\n"
     ]
    }
   ],
   "source": [
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
    "            for i in range(len(test_sentence) - 2)]\n",
    "chunk_len=len(trigrams)\n",
    "print(trigrams[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(test_sentence)\n",
    "voc_len=len(vocab)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=[]\n",
    "tar=[]\n",
    "for context, target in trigrams:\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "        inp.append(context_idxs)\n",
    "        targ = torch.tensor([word_to_ix[target]], dtype=torch.long)\n",
    "        tar.append(targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRU model for Text Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size*2, hidden_size, n_layers,batch_first=True,\n",
    "                          bidirectional=False)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden().cuda()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = decoder(inp[c].cuda(), hidden)\n",
    "        loss += criterion(output, target[c].cuda())\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data.item() / chunk_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2m 30s (100 16%) 0.0001]\n",
      "[4m 58s (200 33%) 0.0001]\n",
      "[7m 24s (300 50%) 0.0001]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 300\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "lr = 0.015\n",
    "\n",
    "decoder = RNN(voc_len, hidden_size, voc_len, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "if(train_on_gpu):\n",
    "    decoder.cuda()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(inp,tar)       \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 50, loss))\n",
    "#         print(evaluate('ge', 200), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f06b13cf4e0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating the text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(prime_str='this process', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden().cuda()\n",
    "\n",
    "    for p in range(predict_len):\n",
    "        \n",
    "        prime_input = torch.tensor([word_to_ix[w] for w in prime_str.split()], dtype=torch.long).cuda()\n",
    "        inp = prime_input[-2:] #last two words as input\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted word to string and use as next input\n",
    "        predicted_word = list(word_to_ix.keys())[list(word_to_ix.values()).index(top_i)]\n",
    "        prime_str += \" \" + predicted_word\n",
    "#         inp = torch.tensor(word_to_ix[predicted_word], dtype=torch.long)\n",
    "\n",
    "    return prime_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this process however afforded mean ascertaining dimension dungeon i might make circuit return point whence i set out without aware fact perfectly uniform seemed wall in left hand gold snuff box which capered hill cutting manner fantastic step took snuff incessantly air\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('this process', 40, temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i might make circuit return point whence i set out without aware fact perfectly uniform seemed wall in left hand gold snuff box which capered hill cutting manner fantastic step took snuff\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('i might', 30, temperature=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Improvement Ideas**\n",
    "\n",
    "As we can see, the model has produced the output which looks fairly fine. The results can be improved further with following points:\n",
    "\n",
    "* Adding more data\n",
    "* Fine Tuning the network architecture\n",
    "* Fine Tuning the network parameters\n",
    "\n",
    "Thanks for going through the notebook, please upvote if you liked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
